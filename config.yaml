trainer:
    accelerator : 'gpu'
    precision: '16-mixed'
    min_epochs: 6
    max_epochs: 100
    benchmark: True
    log_every_n_steps: 10
learning:
    model: "unet"
    loss: "ce"
    backbone: "resnet18"
    weights: "imagenet"
    learning_rate: 1e-3
    learning_rate_schedule_patience: 6
datamodule:
    root: "/home/akramzaytar/ssdprivate/akramz_datasets/wtl"  # Note: put the root where all dataset directories exist (as in `README.md`)
    batch_size: 32
    num_workers: 4
    patch_size: 256
evaluation:
    runs: 3
    sizes: [.01, .05, .1, .25, .5]
